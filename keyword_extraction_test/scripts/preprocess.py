import requests
from bs4 import BeautifulSoup
import os
import re
from velog_scraper import VelogScraper 


def fetch_html(url):
    """
    ì£¼ì–´ì§„ URLì—ì„œ HTML ë‚´ìš©ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜.
    """
    headers = {"User-Agent": "Mozilla/5.0"}
    response = requests.get(url, headers=headers)

    if response.status_code == 200:
        return response.text
    else:
        print(f"âŒ ìš”ì²­ ì‹¤íŒ¨! ìƒíƒœ ì½”ë“œ: {response.status_code}")
        return None


def extract_main_text(html: str) -> str:
    """
    BeautifulSoupìœ¼ë¡œ HTMLì„ íŒŒì‹±í•˜ì—¬ ë³¸ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ìµœëŒ€í•œ ê¹”ë”í•˜ê²Œ ì¶”ì¶œ.
    <div>, <article>, <section> íƒœê·¸ ê¸°ì¤€ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ëª¨ìŒ.
    """
    soup = BeautifulSoup(html, "html.parser")

    content_tags = soup.find_all(['div', 'article', 'section'])
    texts = []

    for tag in content_tags:
        text = tag.get_text(separator=" ", strip=True)
        if text:
            texts.append(text)

    main_text = "\n".join(texts)

    main_text = re.sub(r'[^\w\s]', '', main_text)
    main_text = re.sub(r'\s+', ' ', main_text).strip()

    return main_text


def process_velog_articles():
    """
    Velog íŠ¸ë Œë”© í˜ì´ì§€ì—ì„œ ëª¨ë“  ë¸”ë¡œê·¸ë¥¼ ê°€ì ¸ì™€ ë³¸ë¬¸ì„ ì¶”ì¶œí•˜ê³  ì €ì¥í•˜ëŠ” í•¨ìˆ˜.
    """
    # VelogScraperë¥¼ ì´ìš©í•´ íŠ¸ë Œë”© ë¸”ë¡œê·¸ ë§í¬ ê°€ì ¸ì˜¤ê¸°
    scraper = VelogScraper()
    article_links = scraper.get_trending_links()

    if not article_links:
        print("âŒ ë¸”ë¡œê·¸ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f"âœ… ì´ {len(article_links)}ê°œì˜ ë¸”ë¡œê·¸ë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")

    # í´ë” ìƒì„±
    raw_html_dir = "data/raw_html"
    processed_text_dir = "data/processed_text"
    os.makedirs(raw_html_dir, exist_ok=True)
    os.makedirs(processed_text_dir, exist_ok=True)

    for i, link in enumerate(article_links, start=1):
        print(f"ğŸ“Œ {i}/{len(article_links)} ë¸”ë¡œê·¸ í¬ë¡¤ë§ ì¤‘: {link}")
        
        html_content = fetch_html(link)
        if not html_content:
            continue

        # HTML ì €ì¥ (ì›ë³¸ ë°ì´í„° ë³´ê´€)
        raw_html_path = os.path.join(raw_html_dir, f"blog_{i}.html")
        with open(raw_html_path, "w", encoding="utf-8") as f:
            f.write(html_content)

        # ë³¸ë¬¸ ì¶”ì¶œ & ì „ì²˜ë¦¬
        processed_text = extract_main_text(html_content)

        # í…ìŠ¤íŠ¸ ì €ì¥
        processed_text_path = os.path.join(processed_text_dir, f"blog_{i}.txt")
        with open(processed_text_path, "w", encoding="utf-8") as f:
            f.write(processed_text)

        print(f"âœ… ë³¸ë¬¸ ì €ì¥ ì™„ë£Œ: {processed_text_path}")


# ì‹¤í–‰ ì˜ˆì œ
if __name__ == "__main__":
    process_velog_articles()

    # TODO
    # main text ì¶”ì¶œ ë¡œì§ ìˆ˜ì • í•„ìš”
    # velog ì´ì™¸ ë‹¤ë¥¸ ë¸”ë¡œê·¸ í™•ì¸ í•„ìš”
    # ë¸”ë¡œê·¸ ì´ì™¸ ë‹¤ë¥¸ ì‚¬ì´íŠ¸ í™•ì¸ í•„ìš”
