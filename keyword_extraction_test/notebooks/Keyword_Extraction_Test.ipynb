{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import torch\n",
    "import networkx as nx\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import combinations\n",
    "from konlpy.tag import Okt\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/choiwonjun/nltk_data', '/Users/choiwonjun/.local/share/virtualenvs/keyword_extraction_test-7wgJIzyW/nltk_data', '/Users/choiwonjun/.local/share/virtualenvs/keyword_extraction_test-7wgJIzyW/share/nltk_data', '/Users/choiwonjun/.local/share/virtualenvs/keyword_extraction_test-7wgJIzyW/lib/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.data.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ í˜„ì¬ ì‘ì—… í´ë”: /Users/choiwonjun/nebula/nebula-ai-research/keyword_extraction_test/notebooks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/choiwonjun/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ (TextRankì—ì„œ í•„ìš”)\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "\n",
    "# ë¶„ì„í•  í´ë” ê²½ë¡œ ì„¤ì •\n",
    "TEXT_FOLDER = \"../../data/processed_text\"  # í´ë” ê²½ë¡œ ìˆ˜ì •\n",
    "\n",
    "print(\"ğŸ“‚ í˜„ì¬ ì‘ì—… í´ë”:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì •ê·œì‹ ê¸°ë°˜ í•œêµ­ì–´ì™€ ì˜ì–´ ë¶„ë¦¬ í•¨ìˆ˜\n",
    "def separate_korean_english(text):\n",
    "    \"\"\" í•œêµ­ì–´ì™€ ì˜ì–´ ë¬¸ì¥ì„ ë¶„ë¦¬í•˜ëŠ” í•¨ìˆ˜ \"\"\"\n",
    "    korean_text = \" \".join(re.findall(r'[ê°€-í£]+', text))\n",
    "    english_text = \" \".join(re.findall(r'[a-zA-Z]+', text))\n",
    "    return korean_text, english_text\n",
    "\n",
    "# í•œêµ­ì–´ ë¶ˆìš©ì–´ ë¡œë“œ í•¨ìˆ˜\n",
    "def load_korean_stopwords(file_name=\"ko_stopwords.txt\"):\n",
    "    stop_words = set()\n",
    "    if os.path.exists(file_name):\n",
    "        with open(file_name, \"r\", encoding=\"utf-8\") as f:\n",
    "            stop_words.update(f.read().splitlines())\n",
    "    \n",
    "    return stop_words\n",
    "\n",
    "# ë¶ˆìš©ì–´ ì œê±° í•¨ìˆ˜\n",
    "def remove_stopwords(tokens, language=\"en\"):\n",
    "    if language == \"en\":\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "    else:\n",
    "        stop_words = load_korean_stopwords()\n",
    "    return [token for token in tokens if token not in stop_words]\n",
    "\n",
    "# í† í°í™” í•¨ìˆ˜\n",
    "def tokenize(text):\n",
    "    okt = Okt()\n",
    "    korean_text, english_text = separate_korean_english(text)\n",
    "\n",
    "    # í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ (ëª…ì‚¬ + ì˜ë¯¸ ìˆëŠ” ë‹¨ì–´ ìœ ì§€)\n",
    "    korean_tokens = [word for word, pos in okt.pos(korean_text) if pos in [\"Noun\", \"Alpha\"]] if korean_text else []\n",
    "    \n",
    "    # ì˜ì–´ í† í°í™”\n",
    "    english_tokens = word_tokenize(english_text) if english_text else []\n",
    "\n",
    "    # ë¶ˆìš©ì–´ ì œê±°\n",
    "    korean_tokens = remove_stopwords(korean_tokens, language='ko')\n",
    "    english_tokens = remove_stopwords(english_tokens, language='en')\n",
    "\n",
    "    return korean_tokens + english_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TF-IDF ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ\n",
    "def extract_keywords_tfidf(text, top_n=5):\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=1000)\n",
    "    tfidf_matrix = vectorizer.fit_transform([text])\n",
    "\n",
    "    scores = tfidf_matrix.toarray()[0]\n",
    "    words = vectorizer.get_feature_names_out()\n",
    "\n",
    "    keyword_scores = sorted(zip(words, scores), key=lambda x: x[1], reverse=True)\n",
    "    return [word for word, score in keyword_scores[:top_n]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TF-IDF ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ - Stopword ì¶”ê°€\n",
    "def extract_keywords_tfidf_with_stopwords(text, top_n=5, chunk_size=500):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=50,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(\n",
    "        tokenizer=tokenize,\n",
    "        token_pattern=None,\n",
    "        ngram_range=(1, 2)\n",
    "    )\n",
    "    \n",
    "    tfidf_matrix = vectorizer.fit_transform(chunks)\n",
    "    scores = np.mean(tfidf_matrix.toarray(), axis=0)  # ê° ì²­í¬ì˜ TF-IDF í‰ê· ê°’ ê³„ì‚°\n",
    "\n",
    "    words = vectorizer.get_feature_names_out()\n",
    "\n",
    "    keyword_scores = sorted(zip(words, scores), key=lambda x: x[1], reverse=True)\n",
    "    return [word for word, score in keyword_scores[:top_n]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TextRank ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ\n",
    "def extract_keywords_textrank(text, top_n=5):\n",
    "    words = word_tokenize(text)\n",
    "    words = [word.lower() for word in words if word.isalnum()]\n",
    "\n",
    "    word_graph = nx.Graph()\n",
    "    for w1, w2 in combinations(set(words), 2):\n",
    "        word_graph.add_edge(w1, w2)\n",
    "\n",
    "    scores = nx.pagerank(word_graph)\n",
    "    sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return [word for word, score in sorted_words[:top_n]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TextRank ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ  - Stopword ì¶”ê°€\n",
    "def extract_keywords_textrank_with_stopwords(text, top_n=5):\n",
    "    words = tokenize(text)\n",
    "    words = [word.lower() for word in words if word.isalnum()]\n",
    "\n",
    "    word_graph = nx.Graph()\n",
    "    for w1, w2 in combinations(set(words), 2):\n",
    "        word_graph.add_edge(w1, w2)\n",
    "\n",
    "    scores = nx.pagerank(word_graph)\n",
    "    sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return [word for word, score in sorted_words[:top_n]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LDA ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ\n",
    "def extract_keywords_lda(text, num_topics=1, top_n=5):\n",
    "    vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "    term_matrix = vectorizer.fit_transform([text])\n",
    "\n",
    "    lda_model = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "    lda_model.fit(term_matrix)\n",
    "\n",
    "    words = vectorizer.get_feature_names_out()\n",
    "    topics = lda_model.components_\n",
    "\n",
    "    topic_keywords = [words[i] for i in topics[0].argsort()[-top_n:]]\n",
    "    return topic_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LDA ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ - Stopword ì¶”ê°€\n",
    "def extract_keywords_lda_with_stopwords(text, num_topics=1, top_n=5):\n",
    "    vectorizer = CountVectorizer(tokenizer=tokenize, token_pattern=None)  \n",
    "    term_matrix = vectorizer.fit_transform([text])\n",
    "\n",
    "    lda_model = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "    lda_model.fit(term_matrix)\n",
    "\n",
    "    words = vectorizer.get_feature_names_out()\n",
    "    topics = lda_model.components_\n",
    "\n",
    "    topic_keywords = [words[i] for i in topics[0].argsort()[-top_n:]]  \n",
    "    return topic_keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords_bert(text, top_n=5):\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"].squeeze().tolist())\n",
    "    token_weights = outputs.last_hidden_state.mean(dim=2).squeeze().detach().numpy()\n",
    "\n",
    "    keyword_scores = sorted(zip(tokens, token_weights), key=lambda x: x[1], reverse=True)\n",
    "    return [word for word, score in keyword_scores[:top_n]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "TOKENIZER_BERT_MULTI = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "MODEL_BERT_MULTI = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "TOKENIZER_KOBERT = AutoTokenizer.from_pretrained(\"monologg/kobert\")\n",
    "MODEL_KOBERT = AutoModel.from_pretrained(\"monologg/kobert\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords_bert_multilingual_with_custom_tokenizer(text, top_n=5):\n",
    "    custom_tokens = tokenize(text)\n",
    "    processed_text = \" \".join(custom_tokens)\n",
    "\n",
    "    inputs = TOKENIZER_BERT_MULTI(processed_text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    \n",
    "    with torch.no_grad():  \n",
    "        outputs = MODEL_BERT_MULTI(**inputs)\n",
    "\n",
    "    tokens = TOKENIZER_BERT_MULTI.convert_ids_to_tokens(inputs[\"input_ids\"].squeeze().tolist())\n",
    "    token_weights = outputs.last_hidden_state.mean(dim=1).squeeze().detach().numpy()\n",
    "\n",
    "    keyword_scores = sorted(zip(tokens, token_weights), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    merged_keywords = []\n",
    "    current_word = \"\"\n",
    "\n",
    "    for word, score in keyword_scores:\n",
    "        if word.startswith(\"##\"):  # ì„œë¸Œì›Œë“œì´ë©´ ê¸°ì¡´ ë‹¨ì–´ì— ë¶™ì´ê¸°\n",
    "            current_word += word[2:]\n",
    "        else:  \n",
    "            if current_word:  # ê¸°ì¡´ ë‹¨ì–´ê°€ ìˆë‹¤ë©´ ì €ì¥\n",
    "                merged_keywords.append(current_word)\n",
    "            current_word = word  # ìƒˆë¡œìš´ ë‹¨ì–´ ì‹œì‘\n",
    "\n",
    "    if current_word:  # ë§ˆì§€ë§‰ ë‹¨ì–´ ì¶”ê°€\n",
    "        merged_keywords.append(current_word)\n",
    "\n",
    "    filtered_keywords = [word for word in merged_keywords if word.isalnum()][:top_n]\n",
    "\n",
    "    return filtered_keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords_bert_multilingual_basic(text, top_n=5):\n",
    "    inputs = TOKENIZER_BERT_MULTI(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "\n",
    "    with torch.no_grad():  \n",
    "        outputs = MODEL_BERT_MULTI(**inputs)\n",
    "\n",
    "    tokens = TOKENIZER_BERT_MULTI.convert_ids_to_tokens(inputs[\"input_ids\"].squeeze().tolist())\n",
    "    token_weights = outputs.last_hidden_state.mean(dim=1).squeeze().detach().numpy()\n",
    "\n",
    "    keyword_scores = sorted(zip(tokens, token_weights), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    merged_keywords = []\n",
    "    current_word = \"\"\n",
    "\n",
    "    for word, score in keyword_scores:\n",
    "        if word.startswith(\"##\"):  # ì„œë¸Œì›Œë“œì´ë©´ ê¸°ì¡´ ë‹¨ì–´ì— ë¶™ì´ê¸°\n",
    "            current_word += word[2:]\n",
    "        else:  \n",
    "            if current_word:  # ê¸°ì¡´ ë‹¨ì–´ê°€ ìˆë‹¤ë©´ ì €ì¥\n",
    "                merged_keywords.append(current_word)\n",
    "            current_word = word  # ìƒˆë¡œìš´ ë‹¨ì–´ ì‹œì‘\n",
    "\n",
    "    if current_word:  # ë§ˆì§€ë§‰ ë‹¨ì–´ ì¶”ê°€\n",
    "        merged_keywords.append(current_word)\n",
    "\n",
    "    filtered_keywords = [word for word in merged_keywords if word.isalnum()][:top_n]\n",
    "\n",
    "    return filtered_keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords_kobert_basic(text, top_n=5):\n",
    "    inputs = TOKENIZER_KOBERT(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "\n",
    "    with torch.no_grad():  \n",
    "        outputs = MODEL_KOBERT(**inputs)\n",
    "\n",
    "    tokens = TOKENIZER_KOBERT.convert_ids_to_tokens(inputs[\"input_ids\"].squeeze().tolist())\n",
    "    token_weights = outputs.last_hidden_state.mean(dim=1).squeeze().detach().numpy()\n",
    "\n",
    "    keyword_scores = sorted(zip(tokens, token_weights), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    merged_keywords = []\n",
    "    current_word = \"\"\n",
    "\n",
    "    for word, score in keyword_scores:\n",
    "        if word.startswith(\"##\"):  # ì„œë¸Œì›Œë“œì´ë©´ ê¸°ì¡´ ë‹¨ì–´ì— ë¶™ì´ê¸°\n",
    "            current_word += word[2:]\n",
    "        else:  \n",
    "            if current_word:  # ê¸°ì¡´ ë‹¨ì–´ê°€ ìˆë‹¤ë©´ ì €ì¥\n",
    "                merged_keywords.append(current_word)\n",
    "            current_word = word  # ìƒˆë¡œìš´ ë‹¨ì–´ ì‹œì‘\n",
    "\n",
    "    if current_word:  # ë§ˆì§€ë§‰ ë‹¨ì–´ ì¶”ê°€\n",
    "        merged_keywords.append(current_word)\n",
    "\n",
    "    filtered_keywords = [word for word in merged_keywords if word.isalnum()][:top_n]\n",
    "\n",
    "    return filtered_keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords_kobert_with_custom_tokenizer(text, top_n=5):\n",
    "    custom_tokens = tokenize(text)  # ì‚¬ìš©ì ì •ì˜ í† í¬ë‚˜ì´ì € ì ìš©\n",
    "    processed_text = \" \".join(custom_tokens)  # í† í°ì„ ë¬¸ìì—´ë¡œ ë³€í™˜\n",
    "\n",
    "    inputs = TOKENIZER_KOBERT(processed_text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "\n",
    "    with torch.no_grad():  \n",
    "        outputs = MODEL_KOBERT(**inputs)\n",
    "\n",
    "    tokens = TOKENIZER_KOBERT.convert_ids_to_tokens(inputs[\"input_ids\"].squeeze().tolist())\n",
    "    token_weights = outputs.last_hidden_state.mean(dim=1).squeeze().detach().numpy()\n",
    "\n",
    "    keyword_scores = sorted(zip(tokens, token_weights), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    merged_keywords = []\n",
    "    current_word = \"\"\n",
    "\n",
    "    for word, score in keyword_scores:\n",
    "        if word.startswith(\"##\"):  # ì„œë¸Œì›Œë“œì´ë©´ ê¸°ì¡´ ë‹¨ì–´ì— ë¶™ì´ê¸°\n",
    "            current_word += word[2:]\n",
    "        else:  \n",
    "            if current_word:  # ê¸°ì¡´ ë‹¨ì–´ê°€ ìˆë‹¤ë©´ ì €ì¥\n",
    "                merged_keywords.append(current_word)\n",
    "            current_word = word  # ìƒˆë¡œìš´ ë‹¨ì–´ ì‹œì‘\n",
    "\n",
    "    if current_word:  # ë§ˆì§€ë§‰ ë‹¨ì–´ ì¶”ê°€\n",
    "        merged_keywords.append(current_word)\n",
    "\n",
    "    filtered_keywords = [word for word in merged_keywords if word.isalnum()][:top_n]\n",
    "\n",
    "    return filtered_keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TEXT_FOLDER' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m bert_data = []\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# ì´ íŒŒì¼ ê°œìˆ˜\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m file_list = [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os.listdir(\u001b[43mTEXT_FOLDER\u001b[49m) \u001b[38;5;28;01mif\u001b[39;00m f.endswith(\u001b[33m\"\u001b[39m\u001b[33m.txt\u001b[39m\u001b[33m\"\u001b[39m)]\n\u001b[32m     17\u001b[39m total_files = \u001b[38;5;28mlen\u001b[39m(file_list)\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mğŸ“‚ ì´ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_files\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mê°œì˜ íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'TEXT_FOLDER' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# ì‹¤í–‰ ì‹œê°„ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸\n",
    "execution_times = []\n",
    "\n",
    "# ê²°ê³¼ ë°ì´í„° ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸\n",
    "data = []\n",
    "tf_idf_data = []\n",
    "textrank_data = []\n",
    "lda_data = []\n",
    "bert_data = []\n",
    "\n",
    "# ì´ íŒŒì¼ ê°œìˆ˜\n",
    "file_list = [f for f in os.listdir(TEXT_FOLDER) if f.endswith(\".txt\")]\n",
    "total_files = len(file_list)\n",
    "\n",
    "print(f\"ğŸ“‚ ì´ {total_files}ê°œì˜ íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.\\n\")\n",
    "\n",
    "for i, filename in enumerate(file_list):\n",
    "    file_path = os.path.join(TEXT_FOLDER, filename)\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    print(f\"ğŸ” [{i+1}/{total_files}] {filename} ì²˜ë¦¬ ì¤‘...\")\n",
    "\n",
    "    # ì‹¤í–‰ ì‹œê°„ ê¸°ë¡ì„ ìœ„í•œ ë”•ì…”ë„ˆë¦¬\n",
    "    file_exec_times = {\"íŒŒì¼ëª…\": filename}\n",
    "    file_start_time = time.time()\n",
    "\n",
    "    # âœ… ê¸°ì¡´ ë°©ì‹\n",
    "    print(\"â³ TF-IDF (ê¸°ì¡´) ì‹¤í–‰ ì¤‘...\")\n",
    "    start = time.time()\n",
    "    tfidf_keywords = extract_keywords_tfidf(text)\n",
    "    file_exec_times[\"TF-IDF (ê¸°ì¡´)\"] = round(time.time() - start, 3)\n",
    "\n",
    "    print(\"â³ TextRank (ê¸°ì¡´) ì‹¤í–‰ ì¤‘...\")\n",
    "    start = time.time()\n",
    "    textrank_keywords = extract_keywords_textrank(text)\n",
    "    file_exec_times[\"TextRank (ê¸°ì¡´)\"] = round(time.time() - start, 3)\n",
    "\n",
    "    print(\"â³ LDA (ê¸°ì¡´) ì‹¤í–‰ ì¤‘...\")\n",
    "    start = time.time()\n",
    "    lda_keywords = extract_keywords_lda(text)\n",
    "    file_exec_times[\"LDA (ê¸°ì¡´)\"] = round(time.time() - start, 3)\n",
    "\n",
    "    print(\"â³ BERT (ê¸°ì¡´) ì‹¤í–‰ ì¤‘...\")\n",
    "    start = time.time()\n",
    "    bert_keywords = extract_keywords_bert(text)\n",
    "    file_exec_times[\"BERT (ê¸°ì¡´)\"] = round(time.time() - start, 3)\n",
    "\n",
    "    print(\"â³ BERT multilingual (ê¸°ì¡´) ì‹¤í–‰ ì¤‘...\")\n",
    "    start = time.time()\n",
    "    bert_multilingual_keywords = extract_keywords_bert_multilingual_basic(text)\n",
    "    file_exec_times[\"BERT multilingual (ê¸°ì¡´)\"] = round(time.time() - start, 3)\n",
    "\n",
    "    print(\"â³ koBERT (ê¸°ì¡´) ì‹¤í–‰ ì¤‘...\")\n",
    "    start = time.time()\n",
    "    kobert_keywords = extract_keywords_kobert_basic(text)\n",
    "    file_exec_times[\"koBERT (ê¸°ì¡´)\"] = round(time.time() - start, 3)\n",
    "\n",
    "    # âœ… í† í¬ë‚˜ì´ì € ì ìš© ë²„ì „\n",
    "    print(\"â³ TF-IDF (í† í¬ë‚˜ì´ì € ì ìš©) ì‹¤í–‰ ì¤‘...\")\n",
    "    start = time.time()\n",
    "    tfidf_keywords_tokenized = extract_keywords_tfidf_with_stopwords(text)\n",
    "    file_exec_times[\"TF-IDF (í† í¬ë‚˜ì´ì € ì ìš©)\"] = round(time.time() - start, 3)\n",
    "\n",
    "    print(\"â³ TextRank (í† í¬ë‚˜ì´ì € ì ìš©) ì‹¤í–‰ ì¤‘...\")\n",
    "    start = time.time()\n",
    "    textrank_keywords_tokenized = extract_keywords_textrank_with_stopwords(text)\n",
    "    file_exec_times[\"TextRank (í† í¬ë‚˜ì´ì € ì ìš©)\"] = round(time.time() - start, 3)\n",
    "\n",
    "    print(\"â³ LDA (í† í¬ë‚˜ì´ì € ì ìš©) ì‹¤í–‰ ì¤‘...\")\n",
    "    start = time.time()\n",
    "    lda_keywords_tokenized = extract_keywords_lda_with_stopwords(text)\n",
    "    file_exec_times[\"LDA (í† í¬ë‚˜ì´ì € ì ìš©)\"] = round(time.time() - start, 3)\n",
    "\n",
    "    print(\"â³ BERT multilingual (í† í¬ë‚˜ì´ì € ì ìš©) ì‹¤í–‰ ì¤‘...\")\n",
    "    start = time.time()\n",
    "    bert_keywords_multilingual_tokenized = extract_keywords_bert_multilingual_with_custom_tokenizer(text)\n",
    "    file_exec_times[\"BERT multilingual (í† í¬ë‚˜ì´ì € ì ìš©)\"] = round(time.time() - start, 3)\n",
    "\n",
    "    print(\"â³ koBERT (í† í¬ë‚˜ì´ì € ì ìš©) ì‹¤í–‰ ì¤‘...\")\n",
    "    start = time.time()\n",
    "    kobert_keywords_tokenized = extract_keywords_kobert_with_custom_tokenizer(text)\n",
    "    file_exec_times[\"koBERT (í† í¬ë‚˜ì´ì € ì ìš©)\"] = round(time.time() - start, 3)\n",
    "\n",
    "    # ğŸ“Œ ì‹¤í–‰ ì‹œê°„ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
    "    execution_times.append(file_exec_times)\n",
    "\n",
    "    # ğŸ“Œ í†µí•© ê²°ê³¼ DataFrameìš©\n",
    "    data.append({\n",
    "        \"íŒŒì¼ëª…\": filename,\n",
    "        \"TF-IDF (ê¸°ì¡´)\": \", \".join(tfidf_keywords),\n",
    "        \"TF-IDF (í† í¬ë‚˜ì´ì € ì ìš©)\": \", \".join(tfidf_keywords_tokenized),\n",
    "        \"TextRank (ê¸°ì¡´)\": \", \".join(textrank_keywords),\n",
    "        \"TextRank (í† í¬ë‚˜ì´ì € ì ìš©)\": \", \".join(textrank_keywords_tokenized),\n",
    "        \"LDA (ê¸°ì¡´)\": \", \".join(lda_keywords),\n",
    "        \"LDA (í† í¬ë‚˜ì´ì € ì ìš©)\": \", \".join(lda_keywords_tokenized),\n",
    "        \"BERT (ê¸°ì¡´)\": \", \".join(bert_keywords),\n",
    "        \"BERT multilingual (ê¸°ì¡´)\": \", \".join(bert_multilingual_keywords),\n",
    "        \"koBERT (ê¸°ì¡´)\": \", \".join(kobert_keywords),\n",
    "        \"BERT multilingual (í† í¬ë‚˜ì´ì € ì ìš©)\": \", \".join(bert_keywords_multilingual_tokenized),\n",
    "        \"koBERT (í† í¬ë‚˜ì´ì € ì ìš©)\": \", \".join(kobert_keywords_tokenized),\n",
    "    })\n",
    "\n",
    "    # ğŸ“Œ ê°œë³„ ì•Œê³ ë¦¬ì¦˜ë³„ DataFrame ì¶”ê°€\n",
    "    tf_idf_data.append({\n",
    "        \"íŒŒì¼ëª…\": filename,\n",
    "        \"TF-IDF (ê¸°ì¡´)\": \", \".join(tfidf_keywords),\n",
    "        \"TF-IDF (í† í¬ë‚˜ì´ì € ì ìš©)\": \", \".join(tfidf_keywords_tokenized),\n",
    "    })\n",
    "\n",
    "    textrank_data.append({\n",
    "        \"íŒŒì¼ëª…\": filename,\n",
    "        \"TextRank (ê¸°ì¡´)\": \", \".join(textrank_keywords),\n",
    "        \"TextRank (í† í¬ë‚˜ì´ì € ì ìš©)\": \", \".join(textrank_keywords_tokenized),\n",
    "    })\n",
    "\n",
    "    lda_data.append({\n",
    "        \"íŒŒì¼ëª…\": filename,\n",
    "        \"LDA (ê¸°ì¡´)\": \", \".join(lda_keywords),\n",
    "        \"LDA (í† í¬ë‚˜ì´ì € ì ìš©)\": \", \".join(lda_keywords_tokenized),\n",
    "    })\n",
    "\n",
    "    bert_data.append({\n",
    "        \"íŒŒì¼ëª…\": filename,\n",
    "        \"BERT (ê¸°ì¡´)\": \", \".join(bert_keywords),\n",
    "        \"BERT multilingual (ê¸°ì¡´)\": \", \".join(bert_multilingual_keywords),\n",
    "        \"koBERT (ê¸°ì¡´)\": \", \".join(kobert_keywords),\n",
    "        \"BERT multilingual (í† í¬ë‚˜ì´ì € ì ìš©)\": \", \".join(bert_keywords_multilingual_tokenized),\n",
    "        \"koBERT (í† í¬ë‚˜ì´ì € ì ìš©)\": \", \".join(kobert_keywords_tokenized),\n",
    "    })\n",
    "\n",
    "    file_end_time = time.time()\n",
    "    print(f\"âœ… {filename} ì²˜ë¦¬ ì™„ë£Œ! ì´ ì†Œìš” ì‹œê°„: {round(file_end_time - file_start_time, 3)}ì´ˆ\\n\")\n",
    "\n",
    "# ğŸ“Š DataFrame ìƒì„±\n",
    "df_results = pd.DataFrame(data)\n",
    "df_tf_idf_results = pd.DataFrame(tf_idf_data)\n",
    "df_textrank_results = pd.DataFrame(textrank_data)\n",
    "df_lda_results = pd.DataFrame(lda_data)\n",
    "df_bert_results = pd.DataFrame(bert_data)\n",
    "\n",
    "# ğŸ“Š ì‹¤í–‰ ì‹œê°„ ë¹„êµ DataFrame\n",
    "df_execution_times = pd.DataFrame(execution_times)\n",
    "\n",
    "print(\"ğŸš€ ëª¨ë“  íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_tf_idf_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_textrank_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_lda_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_bert_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_execution_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ğŸ“Œ ì‹¤í–‰ ì‹œê°„ ë°ì´í„° í™•ì¸\n",
    "display(df_execution_times)\n",
    "\n",
    "# ğŸ“Š ê·¸ë˜í”„ ìŠ¤íƒ€ì¼ ì„¤ì •\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.rcParams[\"axes.labelsize\"] = 12\n",
    "plt.rcParams[\"axes.titlesize\"] = 14\n",
    "plt.rcParams[\"xtick.labelsize\"] = 10\n",
    "plt.rcParams[\"ytick.labelsize\"] = 10\n",
    "\n",
    "# ğŸ“Œ ì‹¤í–‰ ì‹œê°„ ë°ì´í„° ê°€ê³µ\n",
    "df_exec_time_melted = df_execution_times.melt(id_vars=[\"íŒŒì¼ëª…\"], var_name=\"ì•Œê³ ë¦¬ì¦˜\", value_name=\"ì‹¤í–‰ ì‹œê°„\")\n",
    "\n",
    "# âœ… 1. ë§‰ëŒ€ ê·¸ë˜í”„ (Bar Chart) - ì•Œê³ ë¦¬ì¦˜ë³„ í‰ê·  ì‹¤í–‰ ì‹œê°„ ë¹„êµ\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=\"ì•Œê³ ë¦¬ì¦˜\", y=\"ì‹¤í–‰ ì‹œê°„\", data=df_exec_time_melted, ci=None)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.title(\"ì•Œê³ ë¦¬ì¦˜ë³„ í‰ê·  ì‹¤í–‰ ì‹œê°„ ë¹„êµ\")\n",
    "plt.xlabel(\"ì•Œê³ ë¦¬ì¦˜\")\n",
    "plt.ylabel(\"í‰ê·  ì‹¤í–‰ ì‹œê°„ (ì´ˆ)\")\n",
    "plt.show()\n",
    "\n",
    "# âœ… 2. ë°•ìŠ¤ í”Œë¡¯ (Box Plot) - ì‹¤í–‰ ì‹œê°„ì˜ ë³€ë™ì„± í™•ì¸\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x=\"ì•Œê³ ë¦¬ì¦˜\", y=\"ì‹¤í–‰ ì‹œê°„\", data=df_exec_time_melted)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.title(\"ì•Œê³ ë¦¬ì¦˜ë³„ ì‹¤í–‰ ì‹œê°„ ë¶„í¬\")\n",
    "plt.xlabel(\"ì•Œê³ ë¦¬ì¦˜\")\n",
    "plt.ylabel(\"ì‹¤í–‰ ì‹œê°„ (ì´ˆ)\")\n",
    "plt.show()\n",
    "\n",
    "# âœ… 3. íˆíŠ¸ë§µ (Heatmap) - ì‹¤í–‰ ì‹œê°„ ë¹„êµ\n",
    "plt.figure(figsize=(10, 8))\n",
    "df_heatmap = df_execution_times.set_index(\"íŒŒì¼ëª…\")\n",
    "sns.heatmap(df_heatmap, cmap=\"coolwarm\", annot=True, fmt=\".3f\", linewidths=0.5)\n",
    "plt.title(\"ì‹¤í–‰ ì‹œê°„ ë¹„êµ íˆíŠ¸ë§µ\")\n",
    "plt.xlabel(\"ì•Œê³ ë¦¬ì¦˜\")\n",
    "plt.ylabel(\"íŒŒì¼ëª…\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keyword_extraction_test-7wgJIzyW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
